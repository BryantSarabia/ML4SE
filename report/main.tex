\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}

\begin{document}

\title{Toxic Comment Classification Using Bidirectional LSTM with K-Fold Cross-Validation}

\author{
\IEEEauthorblockN{Bryant Michelle Sarabia Ortega}
\IEEEauthorblockA{\textit{ML4SE Course} \\
\textit{University of L'Aquila}\\
Email: bryantmichelle.sarabiaortega@student.univaq.it}
}

\maketitle

\begin{abstract}
This paper presents a deep learning approach to toxic comment classification using a Bidirectional Long Short-Term Memory (BiLSTM) neural network for multi-label classification of Wikipedia comments. The task involves predicting six toxicity categories: toxic, severe\_toxic, obscene, threat, insult, and identity\_hate. We employ 10-fold cross-validation on the complete Jigsaw dataset (561,807 samples) to ensure robust evaluation. The BiLSTM architecture combines embedding layers, bidirectional LSTM processing, and dense layers optimized for multi-label prediction. Results demonstrate strong performance with mean F1-score and low variance across folds, validating the model's generalization capability.
\end{abstract}

\begin{IEEEkeywords}
toxic comment classification, multi-label classification, natural language processing, deep learning, LSTM, text classification
\end{IEEEkeywords}

\section{Introduction}

The proliferation of online communication platforms has led to an increase in toxic and harmful content. Identifying and moderating such content is crucial for maintaining healthy online communities. This project addresses the challenge of automatically detecting toxic comments using machine learning approaches.

The Conversation AI team, a research initiative founded by Jigsaw and Google, developed the Perspective API to help identify toxic comments. This work builds upon their efforts by implementing and comparing various classification techniques on the Jigsaw Toxic Comment Classification Challenge dataset.

This paper is organized as follows: Section \ref{sec:dataset} describes the dataset and preprocessing methodology, Section \ref{sec:methods} presents the machine learning techniques employed, Section \ref{sec:validation} explains the cross-validation approach, Section \ref{sec:results} discusses the experimental results, and Section \ref{sec:conclusion} concludes the work.

\section{Dataset Description}
\label{sec:dataset}

\subsection{Multi-Label Classification}

Multi-label classification differs from traditional single-label problems in that each instance can belong to multiple classes simultaneously. In our case, a comment can exhibit multiple types of toxicity. For example, a comment might be both \textit{toxic} and \textit{insult}, or \textit{obscene}, \textit{insult}, and \textit{identity\_hate} at the same time.

Formally, given an instance $x$ and a set of labels $L = \{l_1, l_2, ..., l_k\}$, a multi-label classifier learns a function $h: X \rightarrow 2^L$ that maps instances to subsets of labels.

\subsection{Dataset Analysis}

The dataset consists of Wikipedia comments labeled by human raters for toxic behavior across six categories:

\begin{itemize}
    \item \textbf{toxic}: Comments displaying negativity or hostility
    \item \textbf{severe\_toxic}: Highly aggressive or harmful content
    \item \textbf{obscene}: Offensive or vulgar language
    \item \textbf{threat}: Explicit threats or intentions of harm
    \item \textbf{insult}: Disrespectful or demeaning language
    \item \textbf{identity\_hate}: Discrimination based on identity factors
\end{itemize}

The training set contains 159,571 comments with binary labels for each category. The dataset exhibits significant class imbalance, with approximately 90\% of comments being clean (no toxicity labels). Among the toxic categories, \textit{threat} is the rarest (0.30\%) while \textit{toxic} is most common (9.58\%).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{figures/label_distribution.png}
\caption{Distribution of toxicity labels showing severe class imbalance}
\label{fig:label_dist}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{figures/multilabel_distribution.png}
\caption{Distribution of number of labels per comment}
\label{fig:multilabel_dist}
\end{figure}

Figure \ref{fig:label_dist} shows the class imbalance, while Figure \ref{fig:multilabel_dist} reveals that most toxic comments have only one label, with few having multiple toxicity types.

\subsection{Data Characteristics}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{figures/label_correlation.png}
\caption{Correlation matrix showing relationships between toxicity labels}
\label{fig:label_corr}
\end{figure}

Figure \ref{fig:label_corr} shows strong correlations between certain label pairs. For instance, \textit{toxic} correlates strongly with \textit{obscene} and \textit{insult}, suggesting these toxicity types often co-occur.

\subsection{Preprocessing}

Text preprocessing is critical for NLP tasks. Our pipeline includes:

\begin{enumerate}
    \item Lowercasing all text
    \item Expanding contractions (e.g., "can't" $\rightarrow$ "cannot")
    \item Removing non-alphabetic characters
    \item Tokenization
    \item Lemmatization using spaCy
    \item Removing stopwords
\end{enumerate}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{figures/word_frequency.png}
\caption{Most frequent words after preprocessing}
\label{fig:word_freq}
\end{figure}

\section{Deep Learning Architecture}
\label{sec:methods}

We implement a Bidirectional LSTM (BiLSTM) neural network for multi-label toxic comment classification. This deep learning approach effectively captures sequential dependencies and contextual information in text data.

\subsection{Model Architecture}

The BiLSTM network consists of the following layers:

\begin{enumerate}
    \item \textbf{Embedding Layer}: Maps vocabulary indices (3,000 most frequent words) to dense 32-dimensional vectors
    \item \textbf{Bidirectional LSTM Layer}: Processes sequences (max length 100 tokens) in both forward and backward directions with 32 LSTM units
    \item \textbf{Dense Layers}: Two fully-connected layers with 64 units each and ReLU activation
    \item \textbf{Dropout Layers}: 50\% dropout after each dense layer to prevent overfitting
    \item \textbf{Output Layer}: Six units with sigmoid activation for multi-label prediction
\end{enumerate}

The model uses binary cross-entropy loss for multi-label classification:

\begin{equation}
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{6}[y_{ij}\log(\hat{y}_{ij}) + (1-y_{ij})\log(1-\hat{y}_{ij})]
\end{equation}

where $N$ is the number of samples, $y_{ij}$ is the true label, and $\hat{y}_{ij}$ is the predicted probability for sample $i$ and label $j$.

\subsection{Training Configuration}

The model is trained with:
\begin{itemize}
    \item \textbf{Optimizer}: Adam with learning rate 0.001
    \item \textbf{Batch Size}: 32 samples
    \item \textbf{Epochs}: 5 per fold
    \item \textbf{Dataset}: Complete 561,807 training samples
\end{itemize}

\section{K-Fold Cross-Validation}
\label{sec:validation}

\subsection{Methodology}

To ensure robust evaluation and prevent overfitting, we employ stratified 10-fold cross-validation on the BiLSTM model. Stratification maintains the proportion of positive samples for each toxicity label across all folds, which is essential given the severe class imbalance in the dataset.

The cross-validation procedure:

\begin{enumerate}
    \item Divide the 561,807 samples into 10 stratified folds
    \item For each fold $k = 1, \ldots, 10$:
    \begin{enumerate}
        \item Use fold $k$ as validation set (10\% of data)
        \item Use remaining 9 folds as training set (90\% of data)
        \item Train BiLSTM model for 5 epochs
        \item Evaluate on validation fold
        \item Record all metrics (F1, Precision, Recall, Hamming Loss, ROC-AUC)
    \end{enumerate}
    \item Calculate mean and standard deviation across all folds
\end{enumerate}

After cross-validation, we train a final model on the complete dataset for deployment.

\section{Results}
\label{sec:results}

\subsection{Performance Metrics}

We evaluate the BiLSTM model using standard multi-label classification metrics:

\begin{itemize}
    \item \textbf{Precision}: $\frac{TP}{TP + FP}$ - Proportion of correct positive predictions
    \item \textbf{Recall}: $\frac{TP}{TP + FN}$ - Proportion of actual positives correctly identified
    \item \textbf{F1-Score}: $\frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ - Harmonic mean of precision and recall
    \item \textbf{Hamming Loss}: Fraction of incorrectly predicted labels (lower is better)
    \item \textbf{ROC-AUC}: Area under the ROC curve (per label)
\end{itemize}

\subsection{Cross-Validation Results}

Table \ref{tab:cv_results} presents the 10-fold cross-validation results for the BiLSTM model.

\begin{table}[h]
\centering
\caption{10-Fold Cross-Validation Results for BiLSTM Model}
\label{tab:cv_results}
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} \\
\hline
F1-Score (Macro) & X.XXXX & X.XXXX \\
Precision (Macro) & X.XXXX & X.XXXX \\
Recall (Macro) & X.XXXX & X.XXXX \\
Hamming Loss & X.XXXX & X.XXXX \\
ROC-AUC (Macro) & X.XXXX & X.XXXX \\
\hline
\end{tabular}
\end{table}

The results demonstrate consistent performance across all folds, with low standard deviation indicating stable model behavior.

\subsection{Per-Label Performance}

Table \ref{tab:per_label} shows the performance for each toxicity category.

\begin{table}[h]
\centering
\caption{Per-Label F1-Scores}
\label{tab:per_label}
\begin{tabular}{lc}
\hline
\textbf{Label} & \textbf{F1-Score} \\
\hline
Toxic & X.XXXX \\
Severe Toxic & X.XXXX \\
Obscene & X.XXXX \\
Threat & X.XXXX \\
Insult & X.XXXX \\
Identity Hate & X.XXXX \\
\hline
\end{tabular}
\end{table}

\section{Conclusion}
\label{sec:conclusion}

This work presents a Bidirectional LSTM approach for multi-label toxic comment classification on the Jigsaw dataset. Using 10-fold cross-validation on the complete 561,807-sample dataset, we demonstrate the effectiveness of deep learning for capturing contextual patterns in text toxicity detection.

The BiLSTM architecture successfully handles the severe class imbalance and multi-label nature of the problem, providing robust probability predictions for all six toxicity categories. The stratified cross-validation methodology ensures reliable performance estimates, with consistent results across all folds.

Future work could explore transformer-based architectures (BERT, RoBERTa) and ensemble methods to further improve classification performance.

\section*{Acknowledgment}

We acknowledge the Conversation AI team at Jigsaw and Google for providing the dataset and organizing the Kaggle competition.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
