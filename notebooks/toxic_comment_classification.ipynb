{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "\n",
    "## Multi-Label Classification using Bidirectional LSTM\n",
    "\n",
    "This notebook implements a deep learning approach for toxic comment classification using BiLSTM with 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We use the complete dataset with 159,571 Wikipedia comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full training dataset\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Label columns: {list(train_df.columns[2:])}\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "### 2.1 Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate label statistics\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "label_counts = train_df[label_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of Toxicity Labels', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Label', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLabel Statistics:\")\n",
    "for label in label_cols:\n",
    "    count = train_df[label].sum()\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"{label:15s}: {count:6d} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Label Distribution\n",
    "\n",
    "Analyze how many labels each comment has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of labels per comment\n",
    "label_counts_per_comment = train_df[label_cols].sum(axis=1)\n",
    "multi_label_dist = label_counts_per_comment.value_counts().sort_index()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "multi_label_dist.plot(kind='bar', color='coral')\n",
    "plt.title('Number of Labels per Comment', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Labels', fontsize=12)\n",
    "plt.ylabel('Number of Comments', fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/multilabel_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMulti-Label Statistics:\")\n",
    "for num_labels, count in multi_label_dist.items():\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"{num_labels} label(s): {count:6d} comments ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Label Correlation\n",
    "\n",
    "Analyze correlations between different toxicity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = train_df[label_cols].corr()\n",
    "\n",
    "# Visualize heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation Between Toxicity Labels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/label_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStrongest Correlations:\")\n",
    "corr_pairs = []\n",
    "for i in range(len(label_cols)):\n",
    "    for j in range(i+1, len(label_cols)):\n",
    "        corr_pairs.append((label_cols[i], label_cols[j], correlation_matrix.iloc[i, j]))\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "for label1, label2, corr in corr_pairs[:5]:\n",
    "    print(f\"{label1} - {label2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comment lengths\n",
    "train_df['comment_length'] = train_df['comment_text'].str.len()\n",
    "train_df['word_count'] = train_df['comment_text'].str.split().str.len()\n",
    "\n",
    "# Compare toxic vs non-toxic lengths\n",
    "toxic_lengths = train_df[train_df['toxic'] == 1]['comment_length']\n",
    "non_toxic_lengths = train_df[train_df['toxic'] == 0]['comment_length']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist([non_toxic_lengths, toxic_lengths], bins=50, label=['Non-toxic', 'Toxic'], \n",
    "         color=['green', 'red'], alpha=0.6)\n",
    "plt.xlabel('Character Count', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Comment Length Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.xlim(0, 1000)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "data_to_plot = [non_toxic_lengths, toxic_lengths]\n",
    "plt.boxplot(data_to_plot, labels=['Non-toxic', 'Toxic'])\n",
    "plt.ylabel('Character Count', fontsize=12)\n",
    "plt.title('Comment Length Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLength Statistics:\")\n",
    "print(f\"Non-toxic comments - Mean length: {non_toxic_lengths.mean():.1f} chars\")\n",
    "print(f\"Toxic comments     - Mean length: {toxic_lengths.mean():.1f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most common words in toxic comments\n",
    "toxic_texts = ' '.join(train_df[train_df['toxic'] == 1]['comment_text'].astype(str))\n",
    "toxic_words = toxic_texts.lower().split()\n",
    "toxic_word_freq = Counter(toxic_words).most_common(20)\n",
    "\n",
    "# Get most common words in non-toxic comments\n",
    "non_toxic_texts = ' '.join(train_df[train_df['toxic'] == 0]['comment_text'].head(10000).astype(str))\n",
    "non_toxic_words = non_toxic_texts.lower().split()\n",
    "non_toxic_word_freq = Counter(non_toxic_words).most_common(20)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "words1, counts1 = zip(*toxic_word_freq)\n",
    "ax1.barh(range(len(words1)), counts1, color='red', alpha=0.6)\n",
    "ax1.set_yticks(range(len(words1)))\n",
    "ax1.set_yticklabels(words1)\n",
    "ax1.set_xlabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Top 20 Words in Toxic Comments', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "words2, counts2 = zip(*non_toxic_word_freq)\n",
    "ax2.barh(range(len(words2)), counts2, color='green', alpha=0.6)\n",
    "ax2.set_yticks(range(len(words2)))\n",
    "ax2.set_yticklabels(words2)\n",
    "ax2.set_xlabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Top 20 Words in Non-Toxic Comments', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/word_frequency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BiLSTM Model Training\n",
    "\n",
    "The BiLSTM model is trained using the `train_models.py` script with 10-fold cross-validation.\n",
    "\n",
    "**Model Configuration:**\n",
    "- Vocabulary size: 3,000 tokens\n",
    "- Sequence length: 100\n",
    "- Embedding dimension: 32\n",
    "- LSTM units: 32 per direction (64 total)\n",
    "- Dense layers: 2 × 64 units with ReLU + Dropout(0.5)\n",
    "- Output: 6 sigmoid units (multi-label)\n",
    "\n",
    "**Training Configuration:**\n",
    "- 10-fold stratified cross-validation\n",
    "- 5 epochs per fold\n",
    "- Batch size: 32\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Loss: Binary cross-entropy\n",
    "\n",
    "**To train the model:**\n",
    "```bash\n",
    "python train_models.py\n",
    "```\n",
    "\n",
    "**Training time:** ~75 minutes on CPU (10 folds × 7.5 min each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Trained Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model metadata\n",
    "import json\n",
    "\n",
    "with open('../models/bilstm_toxic_classifier_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Model name: {metadata['model_name']}\")\n",
    "print(f\"  Trained: {metadata['is_trained']}\")\n",
    "print(f\"  Saved: {metadata['save_timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Results\n",
    "\n",
    "The model was evaluated using stratified 10-fold cross-validation on the complete dataset.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "| Metric | Mean | Std Dev |\n",
    "|--------|------|----------|\n",
    "| **F1-Score (Macro)** | 0.6877 | 0.0143 |\n",
    "| **Precision (Macro)** | 0.7783 | 0.0236 |\n",
    "| **Recall (Macro)** | 0.6282 | 0.0246 |\n",
    "| **Hamming Loss** | 0.0190 | 0.0007 |\n",
    "| **ROC-AUC (Macro)** | 0.9658 | 0.0016 |\n",
    "| **Accuracy** | 0.9172 | 0.0030 |\n",
    "\n",
    "**Key Observations:**\n",
    "- Excellent ROC-AUC (0.9658) indicates strong ranking ability\n",
    "- High precision (0.7783) means 78% of flagged comments are actually toxic\n",
    "- Moderate recall (0.6282) catches 63% of toxic comments\n",
    "- Very low Hamming Loss (0.0190) shows few label prediction errors\n",
    "- Low standard deviations across all metrics indicate stable, robust model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "metrics = ['F1-Score', 'Precision', 'Recall', 'ROC-AUC']\n",
    "means = [0.6877, 0.7783, 0.6282, 0.9658]\n",
    "stds = [0.0143, 0.0236, 0.0246, 0.0016]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(metrics))\n",
    "ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.7, \n",
    "       color='steelblue', capsize=10, error_kw={'linewidth': 2})\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_xlabel('Metric', fontsize=12)\n",
    "ax.set_title('BiLSTM 10-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, mean + std + 0.02, f'{mean:.4f}±{std:.4f}', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/cv_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This project successfully implements a Bidirectional LSTM for multi-label toxic comment classification:\n",
    "\n",
    "**Key Achievements:**\n",
    "- ✓ Trained on complete dataset (159,571 samples)\n",
    "- ✓ Robust 10-fold cross-validation methodology\n",
    "- ✓ Strong performance (F1: 0.69, ROC-AUC: 0.97)\n",
    "- ✓ Low variance across folds (stable model)\n",
    "- ✓ Efficient architecture (1.5MB model size)\n",
    "\n",
    "**Model Characteristics:**\n",
    "- **High Precision:** Conservative model, minimizes false positives\n",
    "- **Good Recall:** Catches majority of toxic content\n",
    "- **Excellent Ranking:** ROC-AUC of 0.97 shows strong discrimination ability\n",
    "\n",
    "**Future Improvements:**\n",
    "- Experiment with transformer architectures (BERT, RoBERTa)\n",
    "- Implement attention mechanisms for interpretability\n",
    "- Ensemble methods combining multiple models\n",
    "- Fine-tune on domain-specific data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
