{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.loader import get_data_loader\n",
    "from src.models.lstm_model import BiLSTMModel\n",
    "from src.evaluation.metrics import MetricsCalculator\n",
    "from src.utils.persistence import ModelPersistence\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_data_loader(mode='file', data_dir='../data')\n",
    "train_df, y_labels = loader.load_train_data()\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Label columns: {list(y_labels.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "label_counts = y_labels[label_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of Toxicity Labels', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Label', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLabel Statistics:\")\n",
    "for label in label_cols:\n",
    "    count = y_labels[label].sum()\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"{label:15s}: {count:6d} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_per_comment = y_labels[label_cols].sum(axis=1)\n",
    "multi_label_dist = label_counts_per_comment.value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "multi_label_dist.plot(kind='bar', color='coral')\n",
    "plt.title('Number of Labels per Comment', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Labels', fontsize=12)\n",
    "plt.ylabel('Number of Comments', fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/multilabel_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMulti-Label Statistics:\")\n",
    "for num_labels, count in multi_label_dist.items():\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"{num_labels} label(s): {count:6d} comments ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = y_labels[label_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation Between Toxicity Labels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/label_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStrongest Correlations:\")\n",
    "corr_pairs = []\n",
    "for i in range(len(label_cols)):\n",
    "    for j in range(i+1, len(label_cols)):\n",
    "        corr_pairs.append((label_cols[i], label_cols[j], correlation_matrix.iloc[i, j]))\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "for label1, label2, corr in corr_pairs[:5]:\n",
    "    print(f\"{label1} - {label2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_length'] = train_df['comment_text'].str.len()\n",
    "train_df['word_count'] = train_df['comment_text'].str.split().str.len()\n",
    "\n",
    "toxic_lengths = train_df[y_labels['toxic'] == 1]['comment_length']\n",
    "non_toxic_lengths = train_df[y_labels['toxic'] == 0]['comment_length']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist([non_toxic_lengths, toxic_lengths], bins=50, label=['Non-toxic', 'Toxic'], \n",
    "         color=['green', 'red'], alpha=0.6)\n",
    "plt.xlabel('Character Count', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Comment Length Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.xlim(0, 1000)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "data_to_plot = [non_toxic_lengths, toxic_lengths]\n",
    "plt.boxplot(data_to_plot, labels=['Non-toxic', 'Toxic'])\n",
    "plt.ylabel('Character Count', fontsize=12)\n",
    "plt.title('Comment Length Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLength Statistics:\")\n",
    "print(f\"Non-toxic comments - Mean length: {non_toxic_lengths.mean():.1f} chars\")\n",
    "print(f\"Toxic comments     - Mean length: {toxic_lengths.mean():.1f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_texts = ' '.join(train_df[y_labels['toxic'] == 1]['comment_text'].astype(str))\n",
    "toxic_words = toxic_texts.lower().split()\n",
    "toxic_word_freq = Counter(toxic_words).most_common(20)\n",
    "\n",
    "non_toxic_texts = ' '.join(train_df[y_labels['toxic'] == 0]['comment_text'].head(10000).astype(str))\n",
    "non_toxic_words = non_toxic_texts.lower().split()\n",
    "non_toxic_word_freq = Counter(non_toxic_words).most_common(20)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "words1, counts1 = zip(*toxic_word_freq)\n",
    "ax1.barh(range(len(words1)), counts1, color='red', alpha=0.6)\n",
    "ax1.set_yticks(range(len(words1)))\n",
    "ax1.set_yticklabels(words1)\n",
    "ax1.set_xlabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Top 20 Words in Toxic Comments', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "words2, counts2 = zip(*non_toxic_word_freq)\n",
    "ax2.barh(range(len(words2)), counts2, color='green', alpha=0.6)\n",
    "ax2.set_yticks(range(len(words2)))\n",
    "ax2.set_yticklabels(words2)\n",
    "ax2.set_xlabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Top 20 Words in Non-Toxic Comments', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/word_frequency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 3000\n",
    "MAX_SEQUENCE = 100\n",
    "EMBEDDING_DIM = 32\n",
    "LSTM_UNITS = 32\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "NUM_FOLDS = 10\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Vocabulary size: {MAX_FEATURES:,} tokens\")\n",
    "print(f\"  Sequence length: {MAX_SEQUENCE}\")\n",
    "print(f\"  Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  LSTM units: {LSTM_UNITS} per direction ({LSTM_UNITS*2} total)\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  K-Fold splits: {NUM_FOLDS}\")\n",
    "print(f\"  Epochs per fold: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "evaluator = MetricsCalculator()\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_df), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold}/{NUM_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    X_train_fold = train_df.iloc[train_idx]['comment_text']\n",
    "    y_train_fold = y_labels.iloc[train_idx]\n",
    "    X_val_fold = train_df.iloc[val_idx]['comment_text']\n",
    "    y_val_fold = y_labels.iloc[val_idx]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_fold):,}\")\n",
    "    print(f\"Validation samples: {len(X_val_fold):,}\")\n",
    "    \n",
    "    model = BiLSTMModel(\n",
    "        max_features=MAX_FEATURES,\n",
    "        max_len=MAX_SEQUENCE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        lstm_units=LSTM_UNITS\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining fold {fold}...\")\n",
    "    model.fit(\n",
    "        X_train_fold, \n",
    "        y_train_fold,\n",
    "        X_val=X_val_fold,\n",
    "        y_val=y_val_fold,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluating fold {fold}...\")\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    metrics = evaluator.calculate_metrics(y_val_fold.values, y_pred)\n",
    "    fold_metrics.append(metrics)\n",
    "    \n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(f\"  Hamming Loss: {metrics['hamming_loss']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics = {}\n",
    "std_metrics = {}\n",
    "for metric in fold_metrics[0].keys():\n",
    "    values = [fold[metric] for fold in fold_metrics]\n",
    "    mean_metrics[metric] = np.mean(values)\n",
    "    std_metrics[metric] = np.std(values)\n",
    "\n",
    "print(\"\\nFINAL RESULTS - 10-Fold Cross-Validation\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMean ± Standard Deviation:\")\n",
    "for metric in mean_metrics.keys():\n",
    "    print(f\"  {metric:20s}: {mean_metrics[metric]:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['F1-Score', 'Precision', 'Recall', 'ROC-AUC']\n",
    "means = [mean_metrics['f1_score'], mean_metrics['precision'], mean_metrics['recall'], mean_metrics['roc_auc']]\n",
    "stds = [std_metrics['f1_score'], std_metrics['precision'], std_metrics['recall'], std_metrics['roc_auc']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(metrics))\n",
    "ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.7, \n",
    "       color='steelblue', capsize=10, error_kw={'linewidth': 2})\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_xlabel('Metric', fontsize=12)\n",
    "ax.set_title('BiLSTM 10-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, mean + std + 0.02, f'{mean:.4f}±{std:.4f}', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../report/figures/cv_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = BiLSTMModel(\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_len=MAX_SEQUENCE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    lstm_units=LSTM_UNITS\n",
    ")\n",
    "\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    train_df['comment_text'], y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training final model on {len(X_train_final):,} samples...\")\n",
    "print(f\"Validation on {len(X_val_final):,} samples...\")\n",
    "\n",
    "final_model.fit(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val_final,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nSaving final model...\")\n",
    "ModelPersistence.create_model_directory('../models')\n",
    "ModelPersistence.save_model_with_metadata(final_model, '../models/bilstm_toxic_classifier.keras')\n",
    "print(\"Model saved to: ../models/bilstm_toxic_classifier.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
